{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: Install packages (skip if already done)\n",
    "# !pip install transformers datasets accelerate --quiet\n",
    "\n",
    "# STEP 1: Imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# STEP 2: Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# GPT-2 has no pad_token by default\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# STEP 3: Prepare your custom dataset\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"Farcaster is a decentralized social protocol.\",\n",
    "        \"Farcaster is open-source social media.\",\n",
    "        \"Farcaster is user-owned social data.\",\n",
    "        \"Farcaster is blockchain-based social.\",\n",
    "        \"Farcaster is censorship-resistant.\",\n",
    "        \"Farcaster is not controlled by big tech.\",\n",
    "        \"Farcaster is where you own your content.\",\n",
    "        \"Farcaster is a new way to socialize online.\",\n",
    "        \"Farcaster is for building social apps.\",\n",
    "        \"Farcaster is an alternative to Twitter.\",\n",
    "        \"Farcaster is growing fast.\",\n",
    "        \"Farcaster is developer-friendly.\",\n",
    "        \"Farcaster is privacy-focused.\",\n",
    "        \"Farcaster is community-driven.\",\n",
    "        \"Farcaster is about digital ownership.\",\n",
    "        \"Farcaster is more than just a feed.\",\n",
    "        \"Farcaster is an evolving ecosystem.\",\n",
    "        \"Farcaster is for the future of social.\",\n",
    "        \"Farcaster is a web3 social layer.\",\n",
    "        \"Farcaster is truly decentralized social.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# STEP 4: Tokenize properly (no padding needed here)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# STEP 5: Group texts into full sequences\n",
    "# Not needed for tiny dataset — you can skip group_texts()\n",
    "\n",
    "# STEP 6: Data collator for CausalLM (no MLM for GPT-style)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# STEP 7: Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./farcaster-model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=30,                         # ⬅️ increased for small dataset\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=1000,                             # ⬅️ not needed to save too often\n",
    "    save_total_limit=1,\n",
    "    logging_steps=5,\n",
    "    report_to=\"none\",\n",
    "    learning_rate=5e-5,                          # ⬅️ you can try slightly lower\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# STEP 8: Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# STEP 9: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# STEP 10: Save trained model\n",
    "trainer.save_model(\"./farcaster-model\")\n",
    "tokenizer.save_pretrained(\"./farcaster-model\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
